{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9deee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Environment parameters\n",
    "grid_size = 100\n",
    "start = (0, 0)\n",
    "goal = (99, 99)\n",
    "num_obstacles = int(grid_size * grid_size * 0.3)\n",
    "\n",
    "# Initialize grid with obstacles\n",
    "grid = np.zeros((grid_size, grid_size))\n",
    "obstacles = set()\n",
    "while len(obstacles) < num_obstacles:\n",
    "    x, y = np.random.randint(0, grid_size), np.random.randint(0, grid_size)\n",
    "    if (x, y) != start and (x, y) != goal:\n",
    "        obstacles.add((x, y))\n",
    "        grid[x, y] = -1  # Mark as obstacle\n",
    "\n",
    "grid[start] = 1  # Start\n",
    "grid[goal] = 2   # Goal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec2b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, grid, start, goal, actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.q_table = np.zeros((grid.shape[0], grid.shape[1], len(actions)))\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.start = start\n",
    "        self.goal = goal\n",
    "        self.grid = grid\n",
    "        self.actions = actions\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(list(self.actions.keys()))\n",
    "        x, y = state\n",
    "        return np.argmax(self.q_table[x, y])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        x, y = state\n",
    "        next_x, next_y = next_state\n",
    "        best_next_action = np.argmax(self.q_table[next_x, next_y])\n",
    "        td_target = reward + self.gamma * self.q_table[next_x, next_y, best_next_action]\n",
    "        self.q_table[x, y, action] += self.alpha * (td_target - self.q_table[x, y, action])\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for episode in range(episodes):\n",
    "            state = self.start\n",
    "            while state != self.goal:\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done = self.transition(state, action)\n",
    "                self.learn(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    def transition(self, state, action):\n",
    "        x, y = state\n",
    "        dx, dy = self.actions[action]\n",
    "        next_state = (x + dx, y + dy)\n",
    "        \n",
    "        if next_state[0] < 0 or next_state[1] < 0 or next_state[0] >= grid_size or next_state[1] >= grid_size:\n",
    "            return state, -1, False\n",
    "        if self.grid[next_state] == -1:\n",
    "            return state, -10, False\n",
    "        if self.grid[next_state] == 2:\n",
    "            return next_state, 10, True\n",
    "        \n",
    "        return next_state, -0.1, False\n",
    "\n",
    "# Define actions\n",
    "actions = {\n",
    "    0: (-1, 0),  # Up\n",
    "    1: (1, 0),   # Down\n",
    "    2: (0, -1),  # Left\n",
    "    3: (0, 1)    # Right\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233a4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Q-learning agent\n",
    "agent = QLearningAgent(grid, start, goal, actions)\n",
    "agent.train(episodes=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5586fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place a new obstacle in the middle of the optimal path\n",
    "new_obstacle = (50, 50)\n",
    "grid[new_obstacle] = -1\n",
    "\n",
    "# Function to reset affected Q-values\n",
    "def reset_q_values_around_obstacle(agent, obstacle):\n",
    "    x, y = obstacle\n",
    "    for action in actions.values():\n",
    "        dx, dy = action\n",
    "        if 0 <= x + dx < grid_size and 0 <= y + dy < grid_size:\n",
    "            agent.q_table[x + dx, y + dy, :] = 0  # Reset Q-values for neighboring cells\n",
    "\n",
    "# Reset Q-values for states affected by the new obstacle\n",
    "reset_q_values_around_obstacle(agent, new_obstacle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9385a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase exploration to promote unlearning\n",
    "agent.epsilon = 0.3  # Increase exploration rate\n",
    "\n",
    "# Retrain the agent to adapt to the new environment\n",
    "agent.train(episodes=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b198b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
